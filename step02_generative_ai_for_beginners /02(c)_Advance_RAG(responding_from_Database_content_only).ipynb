{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFrVugPDEu0unxe/qNg3UP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners%20/02(c)_Advance_RAG(responding_from_Database_content_only).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advance Rag:**\n",
        "\n",
        "Here we will learn to create the RAG pipeline where LLM will only respond from the available dataset into the databases, means it will respond on whatever we have given the inforamtion.\n"
      ],
      "metadata": {
        "id": "IcpHOidkUMgP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "09ncyWh82FCg"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')"
      ],
      "metadata": {
        "id": "CtMzdkGN2c2t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Selection of the right model for advance RAG functionalities:**\n",
        "\n",
        "For the right model selection you can print the available models using below code:\n",
        "\n",
        "The model's usage and limitations will also be mentioned.\n",
        "\n",
        "- When you will navigate to `embedding model 001`, it will be mentioned that it returns the `embed_contents` & its input and output token limitiations will also be mentioned etc.\n",
        "- When you will navigate to `embedding model 004`, You can read all the features of this model as well.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YUSlxkLK39dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "list(genai.list_models())\n"
      ],
      "metadata": {
        "id": "mV-b1QLY2rCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple Embeddings without langchain\n",
        "from typing import Dict\n",
        "\n",
        "result:Dict=genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",  # compulsory\n",
        "    content=\"What is the meanning of life\",                       # compulsory\n",
        "    task_type=\"retrieval_document\",   # compulsory\n",
        "    title=\"Embedding of single string\",  # optional_parameter\n",
        ")\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "b4DNCA6I8lKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(result['embedding'])  # it will print the lenght of embedding content, every content will consist of 768 digit's length"
      ],
      "metadata": {
        "id": "bVWCb93A964b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** If more then one content is required then what will be written in an array like `content=[\"What is the meanning of life\", \"I love Pakistan\"]` and when you will check the lenght the it will return the number of content not the digit's lenght.\n",
        "\n",
        "Lets see via coding example:"
      ],
      "metadata": {
        "id": "HvVpfqh5_n9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple embeddings witohut langchain\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "result_2:Dict=genai.embed_content(\n",
        "    model=\"models/text-embedding-004\",  # compulsory\n",
        "    content=[\"What is the meanning of life\",\n",
        "             \"I love Pakistan\",\n",
        "             \"Pakistan Zindabaad\"\n",
        "             ],  # compulsory\n",
        "    task_type=\"retrieval_document\",   # compulsory\n",
        "    title=\"Embedding of single string\",  # optional_parameter\n",
        ")\n",
        "\n",
        "result_2"
      ],
      "metadata": {
        "id": "ybniif92AoMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the lenght\n",
        "len(result_2['embedding'])"
      ],
      "metadata": {
        "id": "G3CftB1WBAi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the lenght of each content in a list\n",
        "\n",
        "for v in result_2['embedding']:\n",
        "  print(len(v))"
      ],
      "metadata": {
        "id": "Ogp1tYZHBTHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building vector stores, Retrieval using Chroma DB and Langchain:**"
      ],
      "metadata": {
        "id": "5-OYXH7EC7Sh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# working with chroma database (langchain supported)\n",
        "\n",
        "!pip install -U -q langchain-chroma"
      ],
      "metadata": {
        "id": "_lYYnUqqB4lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## for small text you can import Document from the langchain.\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1=Document(\n",
        "    page_content=\"Dogs are great companions, known for their loyalty\",\n",
        "    metadata={\"source\":\"mammal-pets-doc\"}\n",
        ")\n",
        "\n",
        "document_2=Document(\n",
        "    page_content=\"Cats are independent pets that often enjoy their own space\",\n",
        "    metadata={\"source\":\"mammal-pets-doc\"}\n",
        ")\n",
        "\n",
        "documents=[document_1, document_2]\n",
        "\n"
      ],
      "metadata": {
        "id": "TghWDD8mDKpk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e99DimyiE_9e",
        "outputId": "ef4e88f5-254f-4f4f-a7cc-e9b9698b6d79"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# emdedding through langchain\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings=GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")"
      ],
      "metadata": {
        "id": "J-l1xqJzEwJe"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings.embed_query(\"What is the meaning of life?\")"
      ],
      "metadata": {
        "id": "lcWnSj9BF_Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "\n",
        "vector_store=Chroma.from_documents(documents=documents, embedding=embeddings)\n"
      ],
      "metadata": {
        "id": "cKegAqJAZ5Z_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking functions of vector store: these functions will be created automatically.\n",
        "\n",
        "list(dir(vector_store))"
      ],
      "metadata": {
        "id": "JLk0Y00PbHAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.similarity_search(\"tell me about cat\")"
      ],
      "metadata": {
        "id": "xxOyeITycKDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking through async function\n",
        "\n",
        "await vector_store.asimilarity_search(\"tell me about cat\")"
      ],
      "metadata": {
        "id": "mVT1DQhIdDp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.similarity_search_with_score(\"tell me about cat\")"
      ],
      "metadata": {
        "id": "7RwmpwFqdS6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding=embeddings.embed_query(\"cat\")  # convert cat into vector"
      ],
      "metadata": {
        "id": "x17Ha046dbi-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.similarity_search_by_vector(embedding)"
      ],
      "metadata": {
        "id": "sSq1gv0qB7GK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "id": "60iI7HWNCuew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Retriever:**"
      ],
      "metadata": {
        "id": "r2HF4ybGFeK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "retriever =RunnableLambda(vector_store.similarity_search).bind(k=1)\n",
        "\n",
        "retriever.batch(\"cats\")\n"
      ],
      "metadata": {
        "id": "iI9HYSKtC_K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm=ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\",\n",
        "                           api_key=userdata.get(\"GOOGLE_API_KEY\")\n",
        "                           )"
      ],
      "metadata": {
        "id": "Hu4Np8ijI3Oa"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Augment part:**"
      ],
      "metadata": {
        "id": "JVXCX9x9Jpcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# llm will now respond based on the provided document because of the following message.\n",
        "\n",
        "message=\"\"\"\n",
        "Answer this question using the provided context only.\n",
        "{question}\n",
        "\n",
        "context:\n",
        "{context}\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "gJDQ5H1IJtkH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_messages([(\"human\",message)])"
      ],
      "metadata": {
        "id": "5ZBB3xJBLOKG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building RAG:"
      ],
      "metadata": {
        "id": "lZBiwS33McW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "              # Retriever part                                    # Augment  # LLM generation\n",
        "rag_chain={\"context\":retriever, \"question\":RunnablePassthrough()} | prompt | llm"
      ],
      "metadata": {
        "id": "BgmMvoq4MEHs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke(\"tell me about Allama Iqbal\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "smQ9Wl3QMg6f",
        "outputId": "97484222-9ec4-4a95-8a7a-9669c8ab1cac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This document does not contain any information about Allama Iqbal.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke(\"tell me about cats\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "xw5GZ-vwNlNo",
        "outputId": "b61f699a-6192-49a5-d248-ffe63cb1f318",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided text, cats are independent pets that often enjoy their own space.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke(\"tell me about Dogs\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "iXtP834pNqiW",
        "outputId": "71a70c82-3ae7-4ad8-9d34-aa606fca1555",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided text, dogs are great companions known for their loyalty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** We can also invoke from above llm object but that llm will response based on its training data. It will not response based on your Documents/RAG system.\n",
        "\n",
        "**For Example:**\n",
        "\n"
      ],
      "metadata": {
        "id": "2vJOiMKHN6CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm.invoke(\"tell me about Allama Iqbal in one line\")\n",
        "\n",
        "print (response.content)    # Note that llm is not responding based on our data RAG system,"
      ],
      "metadata": {
        "id": "79-Fi_L1OUJ4",
        "outputId": "3bcdbf72-252b-4a7a-b4a5-9d2da70b6bdb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allama Iqbal was a highly influential poet, philosopher, and political activist whose work profoundly shaped the ideology of Pakistan.\n"
          ]
        }
      ]
    }
  ]
}