{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/juKUyeMfWTM89RwG1WEH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/01_langchain_hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Problems/ Limitations of LLM**:\n",
        "\n",
        "1. **For Developers:**\n",
        "\n",
        " - Each company that created chatbots or LLMs used different commands and configurations. This made it hard for developers to remember how to work with each model. **LangChain** solved this problem by providing its own set of tools and commands. Now, developers only need to learn LangChain's tools, and LangChain takes care of managing different LLMs. This has made things much easier for developers.\n",
        "\n",
        "2. **For LLMs:**\n",
        " - The second big problem was related to the LLM, LLMs can only give answers based on the data they were trained on. They can't provide information about specific industries, people, or recent updates.To fix this, **RAG (Retrieval-Augmented Generation)** was introduced. RAG allows LLMs to access external data, like documents or databases, in real time. This helps them give more accurate and up-to-date answers. **We will learn about RAG in next section.**\n",
        "\n",
        "**Lets discuss the Langchain first.**"
      ],
      "metadata": {
        "id": "UzmUz6VAq3po"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain\n",
        "\n",
        "Langchain is basically a framework or a wrapper which can connect any LLM at the back-end.\n",
        "\n",
        "In other words, Langchain is a framework designed to facilitate the development of applications using large language models (LLMs). It provides tools for chaining multiple LLMs and integrating them with external data sources, APIs, and services.\n"
      ],
      "metadata": {
        "id": "dp3czHhqqBcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benefits of using Langchain:\n",
        "\n",
        "\n",
        "Langchain reduces the cognitive load of learning and remembering different coding styles or commands for each LLM, which is a key benefit, other benefits are given below:\n",
        "\n",
        "\n",
        "1. **Works with Different Language Models:**\n",
        " - Langchain can connect to many different language models, so you can try out different ones or switch between them easily. This means you're not stuck with just one type of language model.\n",
        "\n",
        "2. **Chains Models Together:**\n",
        "\n",
        " - You can use Langchain to link multiple language models together, making them work together to solve a problem. This is useful when you need one model to do one thing, and another model to do something else.\n",
        "\n",
        "3. **Improves Performance:**\n",
        "\n",
        " - Langchain helps your applications run more smoothly and efficiently by managing how and when the language model is used. This can save time and reduce costs.\n",
        "\n",
        "4. **Easy for Developers:**\n",
        "\n",
        " - Langchain has an easy-to-use design, so developers can build applications faster and without too much hassle. It simplifies many of the hard parts of working with language models.\n",
        "\n",
        "5. **Support from the Community:**\n",
        "\n",
        " - Langchain has a helpful community of developers and plenty of guides, so if you run into problems, you can find answers or get advice easily. You can also add your own features to Langchain if needed.\n",
        "\n",
        "6. **Unified Interface:**\n",
        "\n",
        " - Langchain provides a consistent interface for working with multiple LLMs. This means you can focus on the logic of your application rather than worrying about the specific quirks of different LLM APIs.\n",
        "\n",
        "7. **Easy Switching Between Models:**\n",
        "\n",
        " - With Langchain, switching from one model to another (like OpenAI GPT to Gemini or others) can be done with minimal changes to your code. Instead of changing the way you interact with the model, you just need to adjust a few settings or configurations.\n",
        "\n"
      ],
      "metadata": {
        "id": "YJt-8KndroKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps of prompting from Langchain:\n",
        "\n",
        " - Steps involved in writting the first `Hello world` in the Langchain, are mentioned below:\n",
        "\n",
        "- However you can read the documentation of lanchain from the link: [LangChain Documentation](https://python.langchain.com/docs/introduction/)\n",
        "\n",
        "1. Install the Langchain and its SDK package.\n",
        "2. Create API keys from the model you wanted to connect with.\n",
        "3. import the API key into colab file.\n",
        "4. import a class `ChatGoogleGenerativeAI` from Langchain's SDK.\n",
        "5. Define the Langchain `model` in a variable so that you may re-use that model.  \n",
        "6. Write a response variable using invoke method.\n",
        "7. print the response using print().   \n"
      ],
      "metadata": {
        "id": "emuPQwr-uTg4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "3SA85bzQl1TZ"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q langchain  # Installing Langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain_google_genai # This package contains the LangChain integrations for Gemini"
      ],
      "metadata": {
        "id": "3rhHrM5L28fq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** \"langchain_google_genai\" package contains a `ChatGoogleGenerativeAI`class which is the recommended way to interface with the Google Gemini series of models.\n",
        "\n",
        "We have imported this class below:"
      ],
      "metadata": {
        "id": "RfeWLfoJyHMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "bKYRAyxc0x1N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY')\n"
      ],
      "metadata": {
        "id": "8D0NRfyf1tti"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting a Model:"
      ],
      "metadata": {
        "id": "lCgSOTHZ_JHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ChatGoogleGenerativeAI(\n",
        "model=\"gemini-2.0-flash-exp\", # specify the model to use\n",
        "temperature=0.3,  # set the randomness of the model's response\n",
        "max_output_tokens=1024,  # set the max token\n",
        " )"
      ],
      "metadata": {
        "id": "nH_Kiw3q2AcR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=model.invoke(\"What is the name of the capital of Russia\")\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJgKiANO5YbZ",
        "outputId": "824deab2-21cd-49e8-b7ea-5d25c45cdf7f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of Russia is **Moscow**.\n"
          ]
        }
      ]
    }
  ]
}