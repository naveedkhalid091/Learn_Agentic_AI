{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUZE9Jv/nDh6Feo+K22KFU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02(b)_updated_RAG_implementation_with_PineconeDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of RAG projects:**\n",
        "\n",
        "For RAG projects in langchain, you need to store and retreive your data.\n",
        "\n",
        "You need the **following environment** to set in your project.\n",
        "\n",
        "1. Install the langchain in your project for creating flexibility in switching the chat models.\n",
        "2. Firstly, you need a database for data storage and its access key.  \n",
        "3. An Embedding model for vectorization of your data.\n",
        "4. LLM model for conversations and its access key.\n",
        "\n",
        "Lets Install the above environment first."
      ],
      "metadata": {
        "id": "5JOE76agyfgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain"
      ],
      "metadata": {
        "id": "LpJ9k93a0lLw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-pinecone"
      ],
      "metadata": {
        "id": "KolMKrKS05QZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain_google_genai"
      ],
      "metadata": {
        "id": "BYOfGO2CPzX4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gettting Access of `PINECONE` & `GEMINI` using API Keys:**"
      ],
      "metadata": {
        "id": "UFfzN7fEEafd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY') # Getting access of PINECONE Database\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY') # Getting access of Gemini"
      ],
      "metadata": {
        "id": "fjzbDljB1Vi3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialization of Pinecone client:**\n",
        "\n",
        "Initializing the `Pinecone client (pc)` is the important step as this client allows you to perform various operations such as creating indexes, inserting vectors, and executing queries."
      ],
      "metadata": {
        "id": "9jGtlFu9LiyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc=Pinecone()"
      ],
      "metadata": {
        "id": "oZCXiBixMIY1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create an Index in PINECONE using above client**."
      ],
      "metadata": {
        "id": "O6YT6TnZFEDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can optionally check the existing index name using be below code to prevent duplicates:**\n",
        "\n",
        "* **i)** First check if the index already exist with the same choosen name.\n",
        "\n",
        "* **ii)** Secondly create an index if the same name index is not already created."
      ],
      "metadata": {
        "id": "YCyVFXy9F1Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i) Checking the index name if it is already exist?\n",
        "\n",
        "existing_indexes=[]\n",
        "\n",
        "checking_db_indexes=pc.list_indexes()\n",
        "print(existing_indexes)\n",
        "\n",
        "for info_index in checking_db_indexes:\n",
        "  existing_indexes.append(info_index.name)\n",
        "\n",
        "print(existing_indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDotj3-qF01_",
        "outputId": "5b477fac-7936-4a11-9717-d6dde14dac8f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['online-rag-project', 'my-9th-chem-book', 'second-rag-project', 'family-structure']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii) Creation of Index\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "\n",
        "index_name=\"my-family\"\n",
        "\n",
        "if index_name in existing_indexes:\n",
        "  print(f\"Index {index_name} already exist\")\n",
        "else:\n",
        "  # PROCEED WITH INDEX CREATION\n",
        "  pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=786,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        ")"
      ],
      "metadata": {
        "id": "fBzpf3pZFXC4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Accessing the above created Index:**\n",
        "\n",
        "- Accessing the index will help us inserting the vectors/embedded data through the below line."
      ],
      "metadata": {
        "id": "_VsIGzMuJsXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=pc.Index(index_name)"
      ],
      "metadata": {
        "id": "MuXSc6fgJvPL"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** PINECONE database setup is successfully completed. Now you need to setup embedding model for vectorization and chunking of your data.\n",
        "\n",
        "You can also varify your created index into the PINECONE database by signing into your database and navigate to **`Database->Indexes`**.   "
      ],
      "metadata": {
        "id": "HqvgL-V1MuYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Select Embedding model:**\n",
        "\n",
        "This model will first ensure that all of your data has been vectorized (converted into numbers) and ready for entering into the Pinecone database through above **`index`** variable.\n",
        "\n",
        "The Embedding model can be selected/imported from the `langchain_google_genai` library as below:   "
      ],
      "metadata": {
        "id": "rGoUN-vfN9V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "E0TvFIU3PJUR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**At this stage, the database is setup and embedding model is also selected for the vectorization of data, finally the vecotrized data will enter into the Pinecone database**\n",
        "\n",
        "The data that need to be vectorized consist of either simple `text`, `small file` or a `large file`.\n",
        "\n",
        "The **`simple text`** & **`small file`** will not be chunked but the **`large files`** will first went through the chunking process and then after chunking, the vectorization will be done.  \n",
        "\n",
        "Lets run all the possiblities one by one."
      ],
      "metadata": {
        "id": "DjpRNIPvM4Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Converting small text into embeddings/vectors**:\n",
        "\n",
        "- We can **convert text into vectors** through the `embed_query` method as follow:\n",
        "\n"
      ],
      "metadata": {
        "id": "fHgqYA8aN9-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_text=embedding_model.embed_query(\"Hello, I am Naveed\")\n",
        "\n",
        "print(vector_text[:5]) # printing the results of only first four embeddings."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LTygBTAOFMM",
        "outputId": "7397fa21-6f74-40ac-ad6e-8384dce5339d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.0330955907702446, 0.019804246723651886, -0.03788883239030838, -0.04886496812105179, 0.04226971045136452]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Adding above `vector_text` into your PINECONE database using `vector_store`.** While the `PineconeVectorStore` is a class of the LangChain framework that simplifies the process of `storing` and `retrieving` vector embeddings in Pinecone."
      ],
      "metadata": {
        "id": "_snAgMxyYCw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store=PineconeVectorStore(index=index, embedding=embedding_model)"
      ],
      "metadata": {
        "id": "sH-frb38ZVo5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**While:**\n",
        "- The `index` parameter tells the vector store where to store and retrieve the vector embeddings.\n",
        "- The embedding parameter defines how the textual data is converted into vectors."
      ],
      "metadata": {
        "id": "risW6_EMbUv_"
      }
    }
  ]
}