{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5zYN+AdyRgUDoZDqUVJU6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02(b)_updated_RAG_implementation_with_PineconeDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of RAG projects:**\n",
        "\n",
        "For RAG projects in langchain, you need to store and retreive your data.\n",
        "\n",
        "You need the **following environment** to set in your project.\n",
        "\n",
        "1. Install the langchain in your project for creating flexibility in switching the chat models.\n",
        "2. Firstly, you need a database for data storage and its access key.  \n",
        "3. An Embedding model for vectorization of your data.\n",
        "4. LLM model for conversations and its access key.\n",
        "\n",
        "Lets Install the above environment first."
      ],
      "metadata": {
        "id": "5JOE76agyfgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain"
      ],
      "metadata": {
        "id": "LpJ9k93a0lLw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-pinecone"
      ],
      "metadata": {
        "id": "KolMKrKS05QZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain_google_genai"
      ],
      "metadata": {
        "id": "BYOfGO2CPzX4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gettting Access of `PINECONE` & `GEMINI` using API Keys:**"
      ],
      "metadata": {
        "id": "UFfzN7fEEafd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY') # Getting access of PINECONE Database\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY') # Getting access of Gemini"
      ],
      "metadata": {
        "id": "fjzbDljB1Vi3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialization of Pinecone client:**\n",
        "\n",
        "Initializing the `Pinecone client (pc)` is the important step as this client allows you to perform various operations such as creating indexes, inserting vectors, and executing queries."
      ],
      "metadata": {
        "id": "9jGtlFu9LiyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc=Pinecone()"
      ],
      "metadata": {
        "id": "oZCXiBixMIY1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create an Index in PINECONE using above client**."
      ],
      "metadata": {
        "id": "O6YT6TnZFEDr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You can optionally check the existing index name using be below code to prevent duplicates:**\n",
        "\n",
        "* **i)** First check if the index already exist with the same choosen name.\n",
        "\n",
        "* **ii)** Secondly create an index if the same name index is not already created."
      ],
      "metadata": {
        "id": "YCyVFXy9F1Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# i) Checking the index name if it is already exist?\n",
        "\n",
        "existing_indexes=[]\n",
        "\n",
        "checking_db_indexes=pc.list_indexes()\n",
        "print(existing_indexes)\n",
        "\n",
        "for info_index in checking_db_indexes:\n",
        "  existing_indexes.append(info_index.name)\n",
        "\n",
        "print(existing_indexes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDotj3-qF01_",
        "outputId": "1294a0b3-84fd-4871-d026-33e3980a0e4b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['online-rag-project', 'my-9th-chem-book', 'second-rag-project', 'family-structure']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ii) Creation of Index\n",
        "from pinecone import ServerlessSpec\n",
        "\n",
        "\n",
        "index_name=\"my-books\"\n",
        "\n",
        "\n",
        "if index_name in existing_indexes:\n",
        "  print(f\"Index {index_name} already exist\")\n",
        "else:\n",
        "  # PROCEED WITH INDEX CREATION\n",
        "  pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=768,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        ")"
      ],
      "metadata": {
        "id": "fBzpf3pZFXC4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Accessing the above created Index:**\n",
        "\n",
        "- Accessing the index will help us inserting the vectors/embedded data through the below line."
      ],
      "metadata": {
        "id": "_VsIGzMuJsXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=pc.Index(index_name)"
      ],
      "metadata": {
        "id": "MuXSc6fgJvPL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** PINECONE database setup is successfully completed. Now you need to setup embedding model for vectorization and chunking of your data.\n",
        "\n",
        "You can also varify your created index into the PINECONE database by signing into your database and navigate to **`Database->Indexes`**.   "
      ],
      "metadata": {
        "id": "HqvgL-V1MuYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Select Embedding model:**\n",
        "\n",
        "This model will first ensure that all of your data has been vectorized (converted into numbers) and ready for entering into the Pinecone database through above **`index`** variable.\n",
        "\n",
        "The Embedding model can be selected/imported from the `langchain_google_genai` library as below:   "
      ],
      "metadata": {
        "id": "rGoUN-vfN9V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embedding_model=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "E0TvFIU3PJUR"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**At this stage, the database is setup and embedding model is also selected for the vectorization of data, finally the vecotrized data will enter into the Pinecone database**\n",
        "\n",
        "The data that need to be vectorized consist of either simple `text`, `small file` or a `large file`.\n",
        "\n",
        "The **`simple text`** & **`small file`** will not be chunked but the **`large files`** will first went through the chunking process and then after chunking, the vectorization will be done.  \n",
        "\n",
        "Lets run all the possiblities one by one."
      ],
      "metadata": {
        "id": "DjpRNIPvM4Nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import PineconeVectorStore**:\n",
        "\n",
        "- The **`PineconeVectorStore`** is a class of the LangChain framework that not only **embed your files automatically** before storing the files into vector databases but it also simplifies the process of `storing` and `retrieving` vector embeddings (the text of your file).  \n",
        "\n",
        "- However, you can **convert text into vectors** manually through the `embed_query` method as follow:\n",
        "\n",
        " `vector_text=embedding_model.embed_query(\"Hello, I am Naveed\")`\n",
        "  `print(vector_text)`\n",
        "\n",
        "But This manual effort has been eliminated by the vector store:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_snAgMxyYCw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "## create a vector store client\n",
        "vector_store=PineconeVectorStore(index=index, embedding=embedding_model)"
      ],
      "metadata": {
        "id": "sH-frb38ZVo5"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**While:**\n",
        "- The `index` parameter tells the vector store where to store and retrieve the vector embeddings.\n",
        "- The embedding parameter defines how the textual data is converted into vectors."
      ],
      "metadata": {
        "id": "risW6_EMbUv_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **Prepare Documents for the upload**\n",
        "\n",
        " - Import the Document from `langchain_core.documents`.\n",
        " - Create a `Document` Object which contains the link of text/file you wanted to store into the Database.\n",
        " - Rather then writting the manual IDs for each document, you can import the `uuid` library for generating the random and unique IDs for each document.\n",
        "\n",
        "The relevent coding these steps is given below:    "
      ],
      "metadata": {
        "id": "2sV2p4Mc22Sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1=Document(\n",
        "    page_content=\"Chemistry book\",\n",
        "    metadata={\"/content/Chemistry 10.pdf\":\"Chemistry Book\"} # path of the file and its title in dictionary\n",
        ")\n",
        "\n",
        "\n",
        "# put all the ducments into an array because only array is accepted while adding the documents in below step\n",
        "documents=[document_1]"
      ],
      "metadata": {
        "id": "NkUPRRlu4uy2"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "QaT749-49-9g",
        "outputId": "37040a92-7aaf-4aba-c540-b477502c47ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add document into Database after importing `uuid` library"
      ],
      "metadata": {
        "id": "uA9AsxLh_BQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)\n"
      ],
      "metadata": {
        "id": "qFvLK39w7mSe",
        "outputId": "c57d1034-f3df-41a7-cf5f-2a7a5cc12eaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['5f77ac62-1334-4d1f-babb-2b1ed38fdb43']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Querying from LLM regarding the document uploaded to the vector database**:\n",
        "\n",
        "We cannot directly ask the large language model (LLM) to get information from the vector database. Instead, we follow these steps:\n",
        " - 1. **Fetch Relevant Data from database:** First, we use a function called similarity_search to find the most relevant data from the vector database based on the user's query.\n",
        " - 2. **Combine Query and Data:** Next, we create a function that combines the user's query with the relevant data fetched from the database. This helps the LLM understand what information is needed.\n",
        " - 3. **Send to LLM:** Finally, we send the combined data (the query and the relevant database information) to the LLM. The LLM will then give a response based on this combined information.  \n",
        "\n",
        "This approach ensures that the LLM can respond accurately, using both the query and the relevant data retrieved from the database."
      ],
      "metadata": {
        "id": "gzcbmRrATxNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## similarity search\n",
        "\n",
        "query= \"Book\"\n",
        "\n",
        "db_result=vector_store.similarity_search(\n",
        "    query,\n",
        "    k=10,\n",
        "    )\n",
        "\n",
        "for res in db_result:\n",
        "  print(f\"content:{res.page_content}\\nMetadata:{res.metadata}\\n\")"
      ],
      "metadata": {
        "id": "h2t58Tz0V-1-",
        "outputId": "5ca839f7-71d7-4e20-f3c0-87e8748967be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content:Chemistry book\n",
            "Metadata:{'/content/Chemistry 10.pdf': 'Chemistry Book'}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# llm calling\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=1000,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "EDJ4bU5_X_xl"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a function for final answer from LLM:"
      ],
      "metadata": {
        "id": "cf1u0BVkYUIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function\n",
        "def answer_to_user(query:str):\n",
        "  # Vector Search\n",
        "  vector_results = vector_store.similarity_search(query, k=2)\n",
        "  ## invoking llm\n",
        "  final_answer = llm.invoke (f'''Answer this user Query: {query}, Here are some ref to answer {vector_results}, ''')\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "IT72gTvcYd-O"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calling function with query\n",
        "\n",
        "answer=answer_to_user(\" how many chepters are avaialble in chemistry book\")\n",
        "print(answer.content)"
      ],
      "metadata": {
        "id": "ZLx4cVwTZIjF",
        "outputId": "42ebb48f-2485-4111-e27d-29e10d741c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "The provided text only mentions the existence of a \"Chemistry book\" in a PDF file.  There is no information within the provided context about the number of chapters it contains.  More information from the actual PDF is needed to answer the question.\n"
          ]
        }
      ]
    }
  ]
}