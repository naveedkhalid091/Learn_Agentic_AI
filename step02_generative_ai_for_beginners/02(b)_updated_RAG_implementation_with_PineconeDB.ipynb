{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9/ha/NbjhejuRHs07/69X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02(b)_updated_RAG_implementation_with_PineconeDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementation of RAG projects:**\n",
        "\n",
        "For RAG projects in langchain, you need to store and retreive your data.\n",
        "\n",
        "You need the **following environment** to set in your project.\n",
        "\n",
        "1. Install the langchain in your project for creating flexibility in switching the chat models.\n",
        "2. Firstly, you need a database for data storage and its access key.  \n",
        "3. An Embedding model for vectorization of your data.\n",
        "4. LLM model for conversations and its access key.\n",
        "\n",
        "Lets Install the above environment first."
      ],
      "metadata": {
        "id": "5JOE76agyfgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain"
      ],
      "metadata": {
        "id": "LpJ9k93a0lLw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain-pinecone"
      ],
      "metadata": {
        "id": "KolMKrKS05QZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q langchain_google_genai"
      ],
      "metadata": {
        "id": "BYOfGO2CPzX4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Gettting Access of `PINECONE` & `GEMINI` using API Keys:**"
      ],
      "metadata": {
        "id": "UFfzN7fEEafd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY') # Getting access of PINECONE Database\n",
        "os.environ['GOOGLE_API_KEY']=userdata.get('GOOGLE_API_KEY') # Getting access of Gemini"
      ],
      "metadata": {
        "id": "fjzbDljB1Vi3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Initialization of Pinecone client:**\n",
        "\n",
        "Initializing the `Pinecone client (pc)` is the important step as this client allows you to perform various operations such as creating indexes, inserting vectors, and executing queries."
      ],
      "metadata": {
        "id": "9jGtlFu9LiyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "pc=Pinecone()"
      ],
      "metadata": {
        "id": "oZCXiBixMIY1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Create an Index in PINECONE using above client**."
      ],
      "metadata": {
        "id": "O6YT6TnZFEDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "index_name=\"family-structure\"\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=786,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        ")"
      ],
      "metadata": {
        "id": "fBzpf3pZFXC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Accessing the above created Index:**\n",
        "\n",
        "- Accessing the index will help us inserting the vectors/embedded data through the below line."
      ],
      "metadata": {
        "id": "_VsIGzMuJsXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=pc.Index(index_name)"
      ],
      "metadata": {
        "id": "MuXSc6fgJvPL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** PINECONE database setup is successfully completed. Now you need to setup embedding model for vectorization and chunking of your data.\n",
        "\n",
        "You can also varify your created index into the PINECONE database by signing into your database and navigate to **`Database->Indexes`**.   "
      ],
      "metadata": {
        "id": "HqvgL-V1MuYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Select Embedding model:**\n",
        "\n",
        "This model will first ensure that all of your data has been vectorized (converted into numbers) and ready for entering into the Pinecone database through above **`index`** variable.\n",
        "\n",
        "The Embedding model can be selected/imported from the `langchain_google_genai` library as below:   "
      ],
      "metadata": {
        "id": "rGoUN-vfN9V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
      ],
      "metadata": {
        "id": "E0TvFIU3PJUR"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}