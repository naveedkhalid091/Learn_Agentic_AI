{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGdpjZpTHzj0PrkGzSa7Rh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02(b)_RAG_implementation_with_PineconeDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Implementation with PineConeDB:\n",
        "\n",
        "**Step#1:** Install the necessary environment as follows:  "
      ],
      "metadata": {
        "id": "vrhpEMw49_eN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1hRWYSN9vg2",
        "outputId": "0221fe69-97d8-4c1d-f846-3cb25d9358ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.2 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-pinecone langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step#2:** In this step we need a to initialize the Pinecone Database.\n",
        "\n",
        "So go to the Pinecone Database, create your account and Generate API key and write your API key in colab secrets and finally get access of your vector database through the following code.   \n",
        "\n"
      ],
      "metadata": {
        "id": "cKwpoZGSA9mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from google.colab import userdata\n",
        "pinecone_api_key = userdata.get('PINECONE_API_KEY')\n",
        "\n",
        "pc = Pinecone(api_key=pinecone_api_key)"
      ],
      "metadata": {
        "id": "AX1G11huEmqf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step#3 - Create Indexing in the Pinecone Database:\n",
        "\n",
        "The below code can also be taken from the [PineCone Vector Database](https://www.pinecone.io/). First `Sign-in` and then go to the `Database -> Indexes` section.\n",
        "\n",
        "You can also see the below code in the [PineCone Documentation](https://python.langchain.com/docs/integrations/vectorstores/pinecone/) by scrolling down to the Initialization Section."
      ],
      "metadata": {
        "id": "7Xc0rpNxGE5w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "index_name = \"online-rag-project\"  # change if desired\n",
        "\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "\n",
        "pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768, # Replace with your model dimensions\n",
        "        metric=\"cosine\", # Replace with your model metric\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        ")\n"
      ],
      "metadata": {
        "id": "ZflzyWF3G3J4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "E0uBIkXKfjNB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our Pinecone index is sucessfully setup, You can also varify this created Index (`online-rag-project`) into the PineCone Database and navigate to `Database -> Indexes`. After creating index, now all data will be stored in this `Index`.  \n",
        "\n",
        "\n",
        "Now, we need to select the embedding model as we had studied that the data can only be saved in vector form so we need to select the embedding model (which will convert our data into vectors/numbers).\n",
        "\n",
        "We will select the google embeddings model in our workings:\n",
        "\n",
        "You can find the Google embedding model from the below mentioned documentation.\n",
        "\n",
        "- [Google Embedding Documentation](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/).\n",
        "\n",
        "\n",
        "This these documentations It is clearly mentioned that you can Connect to Google's generative AI embeddings service using the `GoogleGenerativeAIEmbeddings` class, found in the `langchain-google-genai package` which we have installed at step # 1.\n",
        "\n",
        "But keep in your mind that You also need to get access of Gemini model for accessing the embedding model.\n",
        "\n",
        "So first access the Gemini model and then access the embeddings model as below:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4XSfvSLtNjUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY='GOOGLE_API_KEY'\n",
        "os.environ[GOOGLE_API_KEY]=userdata.get('GOOGLE_API_KEY')\n",
        "\n"
      ],
      "metadata": {
        "id": "dXIP_o1CUT9I"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now embed the model as below:  \n",
        "\n",
        "\n",
        "\n",
        "**Note:** This below code is taken from [Google Embedding Documentation](https://python.langchain.com/docs/integrations/text_embedding/google_generative_ai/), from the `usage` section."
      ],
      "metadata": {
        "id": "eOab0qElV50J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n"
      ],
      "metadata": {
        "id": "ijMyS2M-NpXO"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting text into Embeddings"
      ],
      "metadata": {
        "id": "72FZmqAn3LG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector = embeddings.embed_query(\"hello, world!\")\n",
        "print(vector[:5])"
      ],
      "metadata": {
        "id": "eUogkiOC2_lJ",
        "outputId": "6a97fe0a-9128-474d-c59c-f0f6bf9ac29d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05168594419956207, -0.030764883384108543, -0.03062233328819275, -0.02802734263241291, 0.01813093200325966]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding the embeddings into a wrapper (PineconeVectoreStore) so that developers can switch other databases.\n"
      ],
      "metadata": {
        "id": "bR_mWXGcaKzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vector_store= PineconeVectorStore(index=index, embedding=embeddings)"
      ],
      "metadata": {
        "id": "RnuITl6lYgJ7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add items to vector store (Create Documents):\n",
        "We can add items to our vector store by using the add_documents function and then we will reterive the added data."
      ],
      "metadata": {
        "id": "xxw36PKCfy2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # 1. create Data:\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1=Document(\n",
        "    page_content=\"9th class Chemistry Book\",\n",
        "    metadata={\"source\":\"tweet\"},\n",
        ")\n"
      ],
      "metadata": {
        "id": "mk8HTc-_1uOM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(document_1)"
      ],
      "metadata": {
        "id": "3QU06c5zNwRm",
        "outputId": "d65c967c-881f-4ec7-e315-3dab4ca65a3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='9th class Chemistry Book' metadata={'source': 'tweet'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Save data on vector store:\n",
        "\n",
        "from uuid import uuid4\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "document_1=Document(\n",
        "    page_content=\"9th class Chemistry Book\",\n",
        "    metadata={\"source\":\"Chem Book\"},\n",
        ")\n",
        "\n",
        "document_2=Document(\n",
        "    page_content=\"10th class Physics Book\",\n",
        "    metadata={\"source\":\"physics Book\"},\n",
        ")\n",
        "\n",
        "document_3=Document(\n",
        "    page_content=\"I am feeling sick, I will not attend today's class\",\n",
        "    metadata={\"source\":\"tweet\"},\n",
        ")\n",
        "\n",
        "\n",
        "documents=[\n",
        "    document_1,\n",
        "    document_2,\n",
        "    document_3,\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UfBhA3PxMaWd"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "id": "7pt8ujU8PKUU",
        "outputId": "360833fe-a7bc-4106-a218-5752f14c8d82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "uuid4()  # Generates Random id number every time\n"
      ],
      "metadata": {
        "id": "Z4NaixeUPdFs",
        "outputId": "1064e734-9d55-4449-9f6f-99fbcaabd538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UUID('e8cf9009-a62d-457a-9cf5-1851d68f55c3')"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating IDs of all documents using the for loop method\n",
        "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
        "\n",
        "vector_store.add_documents(documents=documents, ids=uuids)"
      ],
      "metadata": {
        "id": "77Yy9h8AP0in",
        "outputId": "4d2116ea-a2f4-4548-ede9-ff7c39a1efd6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['09d6b130-d2dd-499c-9725-b87eeba80508',\n",
              " '5e4aebcd-f862-4d22-a04a-243d712f61f5',\n",
              " '64fce7f4-bda0-4bda-92bb-24d2d16b7092']"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Similarity search on the vector store:\n",
        "\n",
        "\n",
        "results = vector_store.similarity_search(\n",
        "    \"sick\",\n",
        "    k=2,\n",
        "    filter={\"source\":\"tweet\"}\n",
        "    )\n",
        "\n",
        "for res in results:\n",
        "    print(f'''{res.page_content} [{res.metadata}]''')"
      ],
      "metadata": {
        "id": "iI5MGU_LR-2D",
        "outputId": "1a386661-a6b4-4085-fce5-efa986a7b70a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am feeling sick, I will not attend today's class [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity search with scores:\n",
        "results = vector_store.similarity_search_with_score(\n",
        "    \"Who won the USA Elections?\"\n",
        "    )\n",
        "\n",
        "for res, score in results:\n",
        "    print(f'''[SIM={score:3f}]{res.page_content} [{res.metadata}]''')\n"
      ],
      "metadata": {
        "id": "982Yy04fVUsM",
        "outputId": "391a1fca-1ef4-4a3a-c5ca-66ebddb6ea93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SIM=0.511861]10th class Physics Book [{'source': 'physics Book'}]\n",
            "[SIM=0.506155]9th class Chemistry Book [{'source': 'Chem Book'}]\n",
            "[SIM=0.504221]I am feeling sick, I will not attend today's class [{'source': 'tweet'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is dummy example, we will give the result of GAN to the LLM along with our query,\n",
        "# We will learn about it later on by calling below function and importing a chatGoogleGenAI.\n",
        "\n",
        "\n",
        "# copy and pasted code from\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KNct0EdpWUgW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function\n",
        "def answer_to_user(query:str):\n",
        "  # Vector Search\n",
        "  vector_results = vector_store.similarity_search(query, k=2)\n",
        "  print(len(vector_results))\n",
        "  # TODO: Pass to model Vector Results + User Query\n",
        "  final_answer = llm.invoke (f'''Answer this user Query: {query}, Here are some ref to answer {vector_results}, ''')\n",
        "  return final_answer"
      ],
      "metadata": {
        "id": "U91UFmVlcHwg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer=answer_to_user(\"Sick\")"
      ],
      "metadata": {
        "id": "iBiYD2f7bzNu",
        "outputId": "46b6cd25-27dc-4c51-ac3c-4c1ebad4ff84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer.content"
      ],
      "metadata": {
        "id": "uOS6NfrbdFT2",
        "outputId": "480eed2e-28ad-4767-b90e-bc4a532c9afd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The user is reporting feeling sick and will miss class today.  The provided documents are irrelevant to their statement; one is a tweet stating the same, and the other is a reference to a chemistry textbook.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visit the Loader url link to uplaod the documents in RAG.\n",
        "\n",
        "(Donument Loader in RAG)[https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/]"
      ],
      "metadata": {
        "id": "16D-xYy0e0FR"
      }
    }
  ]
}