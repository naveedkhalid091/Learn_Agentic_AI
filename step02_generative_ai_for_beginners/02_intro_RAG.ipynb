{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdAi/87JSZLQZs+wcL4EzS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02_intro_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is RAG?\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is a tool that enhances AI's responses by combining two components:\n",
        "\n",
        "1. **Retrieval-Based Models:** These fetch relevant data from external sources like APIs, databases, or the web.\n",
        "2. **Generative Models:** These use the retrieved information to generate meaningful and contextually relevant responses.\n",
        "\n",
        "### Why RAG is Important:\n",
        "Without RAG, LLMs rely only on their training data, which may not cover real-time or domain-specific knowledge. RAG bridges this gap by allowing LLMs to access up-to-date or customized information before crafting a response.\n",
        "\n",
        "**Practical Example:**\n",
        "Imagine OpenAI's ChatGPT trained only until October 2023. If you ask it for today's weather, it won't know unless connected to a retrieval system (like an API or database). With RAG, ChatGPT can fetch real-time weather information from the internet or a connected database, integrate it into its response, and provide accurate answers.\n",
        "\n",
        "**Note#1: Please bear in mind that RAG can't search data from the whole internet, it can only search information if the information is stored into a connected database with RAG or connected API.**\n",
        "\n",
        "**Note#2: Secondly, RAG is like a Google in terms of searching point of view but it can only search data from the connected databases/parts or APIs and send the retrieved data to the LLM for conversational and predictive language style output, Google can only search the relevant information from the internet and response in a form of links/websites.**\n",
        "\n"
      ],
      "metadata": {
        "id": "y3iKcsyYAJ9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attaching a file in Prompt:\n",
        "\n",
        "If LLM can respond based on the attaching a file then why RAG was introduced?\n",
        "\n",
        "- **Scalability**: In enterprise or real-time systems, users can't attach files for every query.\n",
        "- **Automation**: RAG systems automate retrieval, reducing user effort.\n",
        "\n",
        "### When to Use file attachment option ans when to use the RAG?\n",
        "\n",
        "| **Scenario**                   | **Use LLM with File Attachments** | **Use RAG**              |\n",
        "|--------------------------------|-----------------------------------|--------------------------|\n",
        "| Small, static data             | Yes                               | No                       |\n",
        "| Large, dynamic, or updated data| No                                | Yes                      |\n",
        "| Personalized one-time analysis | Yes                               | No                       |\n",
        "| Scalable and automated systems | No                                | Yes                      |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eGHyJDfx-SJj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mechanism of Retrieval-Augmented Generation (RAG) in storing and retrieving content from vertical (Industry Specific) databases:\n",
        "\n",
        "The mechanism of Retrieval-Augmented Generation (RAG) in storing and retrieving content from vertical (Industry Specific) databases involves several steps. Here's a detailed breakdown:\n",
        "\n",
        "\n",
        "### 1. **Content Ingestion**:\n",
        "\n",
        "The Ingestion refers to the process of collecting, importing and organizing data of centent from various sources into a centralized system for further processing.\n",
        "\n",
        "The steps of Ingestions are written below:\n",
        "\n",
        " - **Data Collection:**\n",
        "    - Gather content from specialized sources (e.g., medical journals, legal documents, product catalogs).\n",
        "    - Content can include text, images, videos, or structured data.\n",
        " - **Pre-processing:**\n",
        "    - Clean and format the data to remove noise (e.g., redundant text, formatting issues).\n",
        "    - Convert the content into a machine-readable format, such as plain text or JSON.\n",
        " - **Chunking:**\n",
        "   - Split large pieces of content into smaller, manageable chunks (e.g., paragraphs or sections).\n",
        "   - Assign metadata (e.g., tags, timestamps, categories) to each chunk for easier retrieval.\n",
        "   \n",
        "### 2. Embedding Content:\n",
        " - **Vectorization:** The computers can't understand the human readable content so the chunk of content (made in above step) then converted into the numbers (Vectors) using the pre-trained embedding model (e.g OpenAI embeddings or Sentence Transformers). These vectors now capture the semantic meaning of the text, enabling similarity-based retrieval.\n",
        " - **Storing Vectors**: Store the vectorized content in a vector database like Pinecone, Weaviate, or Milvus.  \n",
        "\n",
        "### 3. Indexing for Retrieval:\n",
        "\n",
        " - **Indexing**: The vector database creates an efficient index for fast similarity searches.\n",
        "\n",
        "### 4. Query Processing and Retrieval:\n",
        "\n",
        "- **Query Embedding:** When a user submits a query, it is converted into a vector using the same embedding model used for content.\n",
        "- **Similarity Search**: The query vector is compared against stored vectors in the database. The system retrieves the most similar chunks.\n",
        "\n",
        "### 5. Generating Responses:\n",
        "\n",
        "- The retrieved chunks are passed as context to the LLM. The LLM uses the retrieved information, along with its pre-trained knowledge, to generate a response.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "EYrAvxG2_WfI"
      }
    }
  ]
}