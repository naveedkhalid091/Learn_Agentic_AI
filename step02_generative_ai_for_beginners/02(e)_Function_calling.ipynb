{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZPDA+rsb/5xmqnSq28yPw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02(e)_Function_calling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "o1-VtoxJ5FVg"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "genai.configure(api_key=userdata.get(\"GOOGLE_API_KEY\"))"
      ],
      "metadata": {
        "id": "22ToFi_67Mjr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set up a model with tools:**\n",
        "\n",
        "This example uses 3 functions that control a simple hypothetical lighting system. Using these functions requires them to be called in a specific order. For example, you must turn the light system on before you can change color.\n",
        "\n",
        "While you can pass these directly to the model and let it try to call them correctly, specifying the function_calling_config gives you precise control over the functions that are available to the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jj2rtPy57gjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enable_lights():\n",
        "    \"\"\"Turn on the lighting system.\"\"\"  # telling to LLM regarding this function in tripple qutation.\n",
        "    print(\"LIGHTBOT: Lights enabled.\")\n",
        "\n",
        "\n",
        "def set_light_color(rgb_hex: str):\n",
        "    \"\"\"Set the light color. Lights must be enabled for this to work.\"\"\"   # telling to LLM regarding this function in tripple qutation.\n",
        "    print(f\"LIGHTBOT: Lights set to {rgb_hex}.\")\n",
        "\n",
        "\n",
        "def stop_lights():\n",
        "    \"\"\"Stop flashing lights.\"\"\"   # telling to LLM regarding this function in tripple qutation.\n",
        "    print(\"LIGHTBOT: Lights turned off.\")\n",
        "\n",
        "light_controls = [enable_lights, set_light_color, stop_lights]\n",
        "instruction = \"You are a helpful lighting system bot. You can turn lights on and off, and you can set the color. Do not perform any other tasks.\"\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    \"models/gemini-1.5-pro\",\n",
        "    tools=light_controls,  # Functions calling via tools\n",
        "    system_instruction=instruction\n",
        ")\n",
        "\n",
        "# Now your model is ready for function calling"
      ],
      "metadata": {
        "id": "JchOUYHS7fmo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat=model.start_chat()"
      ],
      "metadata": {
        "id": "rXb1qAXa_92a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Creating a helper function:**\n",
        "\n",
        "- The helper function is to designed to make things easier for developers.\n",
        "- It takes simple, human friendly inputs ( Mode in str, & a list of allowed function names) & automatically converts those provided inputs into the detailed configuration format that Gemini API requires.\n",
        "- This abstraction helps developers avoid manual errors and speed up the process, ensuring the configuration is always correct.\n",
        "\n",
        "\n",
        "Developers after creation of this function only provide the `mode` and a list of allowed function names. The helper functions then automatically transforms these into the detailed structure the API requires.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eYuzH1LdF0nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function\n",
        "from google.generativeai.types import content_types\n",
        "from collections.abc import Iterable\n",
        "\n",
        "\n",
        "def tool_config_from_mode(mode: str, fns: Iterable[str] = ()):\n",
        "    \"\"\"Create a tool config with the specified function calling mode.\"\"\"\n",
        "    return content_types.to_tool_config(\n",
        "        {\"function_calling_config\": {\"mode\": mode, \"allowed_function_names\": fns}}\n",
        "    )"
      ],
      "metadata": {
        "id": "fW4ECovD7Dg4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Modes:**\n",
        "\n",
        "The selection of modes is very important, the developer is basically delegating power to the LLM to use the functions based on the LLM's judgement.\n",
        "\n",
        "**1. `NONE`:** If you have provided the model with tools, but do not want to use those tools(functions) for the current conversational turn, then specify NONE as the mode. `NONE` tells the model not to make any function calls. The model will behave as `none` have been provided in the tools.\n",
        "\n",
        "**2. `Auto`:** When developer write the `Auto` mode it means that the developer is instructing the LLM to use only one function in the allowed list based on your best judgements.  \n",
        "\n",
        "**3. `Any`:** When developer write the `Any` mode it means that the developer is instructing the LLM to use any function (not be bound to a single function selection) in the allowed list."
      ],
      "metadata": {
        "id": "kMdkw9xxLvIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NONE mode:\n",
        "\n",
        "tool_config = tool_config_from_mode(\"none\")\n",
        "\n",
        "response = chat.send_message(\n",
        "    \"Hello light-bot, what can you do?\", tool_config=tool_config\n",
        ")\n",
        "print(response.text)\n",
        "\n"
      ],
      "metadata": {
        "id": "SA6lNUplLuks",
        "outputId": "f518eeaa-9bfd-4920-e325-088b9e652ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can turn lights on and off, and I can set the color of the lights.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AUTO Mode\n",
        "tool_config = tool_config_from_mode(\"auto\")\n",
        "\n",
        "response = chat.send_message(\"Light this place up!\", tool_config=tool_config)\n",
        "print(response.parts)\n",
        "chat.rewind();  # You are not actually calling the function, so remove this from the history."
      ],
      "metadata": {
        "id": "MLpSNOshMhqD",
        "outputId": "2dfa6a1d-e32d-4446-b177-90aff6a8fc6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[function_call {\n",
            "  name: \"enable_lights\"\n",
            "  args {\n",
            "  }\n",
            "}\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Any Mode\n",
        "\n",
        "available_fns = [\"set_light_color\", \"stop_lights\"]\n",
        "\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "response = chat.send_message(\"Make this place PURPLE!\", tool_config=tool_config)\n",
        "print(response.parts[0])"
      ],
      "metadata": {
        "id": "-2fHrUlcNGK-",
        "outputId": "bace8915-2482-4355-f551-808562b88de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function_call {\n",
            "  name: \"set_light_color\"\n",
            "  args {\n",
            "    fields {\n",
            "      key: \"rgb_hex\"\n",
            "      value {\n",
            "        string_value: \"800080\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Automatic function calling**"
      ],
      "metadata": {
        "id": "pfHGAfmYNrbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_fns = [\"enable_lights\"]\n",
        "tool_config = tool_config_from_mode(\"any\", available_fns)\n",
        "\n",
        "auto_chat = model.start_chat(enable_automatic_function_calling=True)\n",
        "auto_chat.send_message(\"It's awful dark in here...\", tool_config=tool_config)"
      ],
      "metadata": {
        "id": "Rv9zeZ6WNuSb",
        "outputId": "a3641bd7-106a-49de-a6dc-b6b35aa3cc4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LIGHTBOT: Lights enabled.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "response:\n",
              "GenerateContentResponse(\n",
              "    done=True,\n",
              "    iterator=None,\n",
              "    result=protos.GenerateContentResponse({\n",
              "      \"candidates\": [\n",
              "        {\n",
              "          \"content\": {\n",
              "            \"parts\": [\n",
              "              {\n",
              "                \"text\": \"Lights enabled.\"\n",
              "              }\n",
              "            ],\n",
              "            \"role\": \"model\"\n",
              "          },\n",
              "          \"finish_reason\": \"STOP\",\n",
              "          \"avg_logprobs\": -0.4979706605275472\n",
              "        }\n",
              "      ],\n",
              "      \"usage_metadata\": {\n",
              "        \"prompt_token_count\": 169,\n",
              "        \"candidates_token_count\": 3,\n",
              "        \"total_token_count\": 172\n",
              "      },\n",
              "      \"model_version\": \"gemini-1.5-pro-002\"\n",
              "    }),\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}