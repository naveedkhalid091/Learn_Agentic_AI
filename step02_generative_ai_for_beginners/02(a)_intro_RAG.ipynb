{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+zmLJPyNmmx+1anVQuZWM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step02_generative_ai_for_beginners/02(a)_intro_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Solution of problem-2 discussed in previous section:**\n",
        "\n",
        "In the previous section, we discussed a key limitation of Language Learning Models (LLMs) which is again mentioned below:\n",
        "\n",
        "**\"LLMs cannot respond effectively to specific content related to people, organizations, or domain-specific information.\"**\n",
        "\n",
        "**Solution:** The above problem is solved by the **Retrieval-Augmented Generation (RAG)** which is mentioned below:"
      ],
      "metadata": {
        "id": "o-FGF9E6tACN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is RAG**?\n",
        "\n",
        "RAG (Retrieval-Augmented Generation) is basically a tool which **enables the LLM to respond to the specific content** of either related to individual or an organization by combining the following components.\n",
        "\n",
        "1. **Retrieving Relevent/Specific Data:**\n",
        " - The relevant data (person or organization specific) is first retrieved/fetched either from the APIs or databases.\n",
        "2. **Augmenting the Query:**\n",
        " - Secondly, the retrieved data along with the user prompt (Augmentation) is sent to the LLM for specific content response.   \n",
        "\n",
        "\n",
        "**Practical Example:**\n",
        "Imagine OpenAI's ChatGPT trained only until October 2023. If you ask it for today's weather, it won't know unless connected to a retrieval system (like an API or database). With RAG, ChatGPT can fetch real-time weather information from the internet or a connected database, integrate it into its response, and provide accurate answers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y3iKcsyYAJ9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Mechanism of Retrieval-Augmented Generation (RAG)**:\n",
        "\n",
        "The mechanism of Retrieval-Augmented Generation (RAG) in storing and retrieving content from vertical (Industry Specific) databases involves several steps.\n",
        "\n",
        "Here's a detailed breakdown:\n",
        "\n",
        "### 1. **Preparing Data for entering into databases**:\n",
        "\n",
        " - The data is not directly stored into databases, the vector databases only store data into vectors (numbers) form.\n",
        "  \n",
        "  - **Step-1**: The data is firstly converted into the smaller chunks for enhance semantic search and retrieval efficiency.\n",
        "  - **Step-2:**. The chunk of data is then sent to embedding models, these models convert the smaller chunks of data into vector (number) forms. These vectors capture the meanings of the text.    \n",
        "\n",
        "## 2. **Saving vector data into databases:**\n",
        "   \n",
        "   - **Step-3:** The **vector forms data** is now entered into the chosen vector databases.\n",
        "   - **Step-4**: Indexes are created by the developers to optimize the search and retrieval process, allowing the system to locate the relevant vectors.\n",
        "\n",
        "## 3. **Retrieving content from Databases:**   \n",
        "  - **Step-5:** The user write the prompt for asking the specific information.\n",
        "  - **Step-6:** The user's query is converted into the vector forms using the same embedding model used earlier.\n",
        "  - **Step-7:** The relevant vectors are retrieved, converted back into text and merged with the user's query context to generate a response using the LLM.\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "EYrAvxG2_WfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Attaching a file in Prompt**:\n",
        "\n",
        "If LLM can respond based on the attaching a file then why RAG was introduced?\n",
        "\n",
        "- **Scalability**: In enterprise or real-time systems, users can't attach files for every query.\n",
        "- **Automation**: RAG systems automate retrieval, reducing user effort."
      ],
      "metadata": {
        "id": "eGHyJDfx-SJj"
      }
    }
  ]
}