## **Integrating Different LLMs with different agents in same project:**

Normally the inoking LLMs can be expensive so we should have a strategy to call the simple, less expensive or free LLMs for simple tasks and for the complex tasks we should call the expensive LLMs.

For Exmaple:

If we have two agents in a project one agent is `junior_devloper` while other is `senior_developer` agent.

We will now integrate both the `deepseekr1:1.7b` & `Gemini-2.0-flash` models with the `junior_developer` & `senior developer` respectively. You should see the project `multiple_agents_with_multiple_LLMs` for coding examples.

First you should write all the code which we had written in `multiple_agents` project because the coding process is same but with additional commands in the yaml file. which are mentioned below:

You should add `llm` and `base_url`, value-key parameter to tell the agent which llm to use.

You need to go to the `agents.yaml` file and add below lines in junior and senior agent. Note it down that all other code will be same as `multi-agents`

```yaml
junior_python_developer:
  role: > # role unchanged
    Junior Python Developer
  goal: > # goal unchanged
    Write python code solution without type hints. for this problem '{problem}'.
  backstory: > # backstory unchanged
    You have 3 years of python web development experience.
    you know how to use flask, django.
  llm:
    >- # only add this key-value parameter, where - indicates to exclude the extra whitespaces
    ollama/deepseek-r1:1.5b
  base_url: >-
    http://localhost:11434

senior_python_developer:
  role: > # unchanged
    Senior Python Developer
  goal: > # unchanged
    review the python code generated by junior python developer for this problem '{problem}'.
    apply type hints to the code.
    apply pydocs
    write 3 pytest tests for the code.
  backstory: > # unchanged
    You have 20 years of experience in Machine Learning, Deep Learning, Generative AI, Agentic AI and Python.
    you have good commands in crewai, langgraph, AutoGen.
  llm:
    >- # only add this key-value parameter, where - indicates to exclude the extra whitespaces
    gemini/gemini-2.0-flash
```

So, there will only be a change in `agents.yaml` file to integrate the llms in your project

I have crated a seperate project fo this llm integration. you can go to `multiple_agents_with_multiple_LLMs` project for further information
