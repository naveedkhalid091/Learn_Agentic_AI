{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LiteLLM: \n",
    "\n",
    "- LiteLLM is lightweight python library that **allow developers to interect with a wide range of Large language model (LLM)** APIs. It lets you call over 100 different LLM providersâ€”such as OpenAI, Anthropic, Azure, Cohere, HuggingFace, and others **using a consistent OpenAI-style interface.**  \n",
    "\n",
    "- Now due to LiteLLM, developers are not required to memorize the different syntax of different LLMs. LiteLLM has provided a unified code to interect with 100+ LLM models. \n",
    "\n",
    "- LiteLLM is basically an open-source package written on the OpenAI's style syntax but you can connect 100+ models. \n",
    "\n",
    "- **Documentation and Features:** LiteLLM has its own documentation, which outlines various features that make it a user-friendly choice for interacting with LLMs. \n",
    "\n",
    "Read the LiteLLM docs for further inforamtion - [Docs](https://docs.litellm.ai/docs/)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Installing liteLLM from `uv project`: \n",
    "\n",
    "- Create a `uv` project as: \n",
    "   `uv init --package projectName`\n",
    "- Navigate to the project directory and run the liteLLM as follows: \n",
    "   `uv run liteLLM`\n",
    "- Navigate to the `.toml` file and varify the installation of `liteLLM` in dependancies array.\n",
    "- Create a file say `hello.py` in `src` folder.\n",
    "- write following code in the `hello.py` file. \n",
    "\n",
    "    ```python \n",
    "    from litellm import completion\n",
    "\n",
    "    import os\n",
    "\n",
    "    os.environ['GEMINI_API_KEY'] = \"YOUR_API_KEY\"\n",
    "\n",
    "    def call_gemini():\n",
    "        response = completion(\n",
    "        model=\"gemini/gemini-1.5-flash\", \n",
    "        messages=[{\"role\": \"user\", \"content\": \"Who is Quaid e Azam\"}]\n",
    "        )\n",
    "        print(response['choices'][0]['message']['content']) # this method is provided in liteLLM docs. \n",
    "    ```\n",
    "\n",
    "- Write a variable in `.toml` file for creating a package in following syntax.\n",
    "\n",
    "    `variable=projectPath:functionName`\n",
    "\n",
    "- Run the command in terminal to get the response. \n",
    "\n",
    "**Note:** You can also try the different models (i.e. Gemini-2.0-exp) in the above code because liteLLM does support all the LLM models by writting the same code. Please refer to the `Project1` to see the code. \n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
