{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3p+1CSDvcyPIVOPit8Suz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naveedkhalid091/Learn_Agentic_AI/blob/main/step00_hello_world/01(d)_chatSession_prompting_method_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Session:\n",
        "\n",
        "Chat session enables the LLM to remeber the history of prompts or previous conversations:\n",
        "\n"
      ],
      "metadata": {
        "id": "kibESxtffLuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IAe5-vj0fKI6"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"google-generativeai>=0.7.2\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "R-KApNHtgXBV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "pB4GIMyvghUV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=genai.GenerativeModel(\"gemini-2.0-flash-exp\")"
      ],
      "metadata": {
        "id": "6_Cb2SM7g9s5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat=model.start_chat(history=[])\n",
        "print(chat.history)  # currently the history is empty"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fut1KsIskQFv",
        "outputId": "2f516f73-28fb-4b74-d58d-c04bdfa0b640"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat.send_message(\"Who is the current prime minister of Pakistan\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ZwJUY7MEk-7r",
        "outputId": "eb518a7f-b111-4bff-a333-8b3d1cf39e8d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The current Prime Minister of Pakistan is **Anwar ul Haq Kakar**. He is serving as the caretaker prime minister since August 14, 2023.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=chat.send_message(\"What is his qualification, comment in one line\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "pqbHSXQhl2eW",
        "outputId": "38701888-c3f3-4648-fbf6-aaf600d1ab40"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He has a Master's degree in Political Science and Sociology, and has served as a Senator.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat.history) # Now history of prompts along with responses is maintained"
      ],
      "metadata": {
        "id": "EU08IeUElPjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:**\n",
        "- You can prompt using the model.generate_content() and using the chat.send.message() method. **model.generate_content()** is useful for the single-turn interactions but **chat.send.message()** is used for the chat session context.\n",
        "\n",
        "- In the chat session context the history of previous messages and responses is preserved. **\n",
        "\n",
        "**For example:** The second prompt understands that \"his\" refers to \"Prime Minister's qualification\" because the history is maintained."
      ],
      "metadata": {
        "id": "Bv-GtUHCocpD"
      }
    }
  ]
}